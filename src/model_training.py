from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score, f1_score, classification_report, precision_score, recall_score, roc_curve, auc, ConfusionMatrixDisplay, confusion_matrix
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from model_manager import ModelManager
from datetime import datetime
import os

class TrainModels:
    def __init__(self, X_train, X_test, y_train, y_test):
        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.y_test = y_test
        self.models = {}
        self.predictions = {}
        self.model_manager = ModelManager()
    
    def fit_models(self,):
        """ –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ """
        logistic_regression = LogisticRegression(random_state=42, max_iter=1000)
        logistic_regression.fit(self.X_train, self.y_train)
        self.models['LogisticRegression'] = logistic_regression

        knn = KNeighborsClassifier()
        knn.fit(self.X_train, self.y_train)
        self.models['KNeighborsClassifier'] = knn

        decision_tree = DecisionTreeClassifier(random_state=42)
        decision_tree.fit(self.X_train, self.y_train)
        self.models['DecisionTreeClassifier'] = decision_tree

        random_forest = RandomForestClassifier(random_state=42)
        random_forest.fit(self.X_train, self.y_train)
        self.models['RandomForestClassifier'] = random_forest

        xgboost = XGBClassifier(random_state=42)
        xgboost.fit(self.X_train, self.y_train)
        self.models['XGBClassifier'] = xgboost
        
        lightgbm = LGBMClassifier(random_state=42, verbose=0)
        lightgbm.fit(self.X_train, self.y_train)
        self.models['LGBMClassifier'] = lightgbm

        catboost = CatBoostClassifier(random_state=42, verbose=0)
        catboost.fit(self.X_train, self.y_train)
        self.models['CatBoostClassifier'] = catboost

        return self.models
    
    def add_tuned_models(self, tuned_models_dict):
        """ –î–æ–±–∞–≤–ª—è–µ—Ç –∫ –æ–±—â–µ–º—É —Å–ø–∏—Å–∫—É –º–æ–¥–µ–ª–µ–π –Ω–∞—à–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏ """
        for name, model in tuned_models_dict.items():
            model.fit(self.X_train, self.y_train)
            self.models[name] = model
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(tuned_models_dict)} –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
        return self.models
    
    def evaluate_models(self):
        """ –°—á–∏—Ç–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏ """
        for name_model, model in self.models.items():
            y_pred_test = model.predict(self.X_test)
            y_pred_proba_test = model.predict_proba(self.X_test)[:, 1]

            y_pred_train = model.predict(self.X_train)
            y_pred_proba_train = model.predict_proba(self.X_train)[:, 1]
            self.predictions[name_model] = {
                "test_roc_auc": roc_auc_score(self.y_test, y_pred_proba_test),
                "test_f1_score": f1_score(self.y_test, y_pred_test),
                "test_precision": precision_score(self.y_test, y_pred_test),
                "test_recall": recall_score(self.y_test, y_pred_test),
                
                "train_roc_auc": roc_auc_score(self.y_train, y_pred_proba_train),
                "train_f1_score": f1_score(self.y_train, y_pred_train),
                "train_precision": precision_score(self.y_train, y_pred_train),
                "train_recall": recall_score(self.y_train, y_pred_train),

                "roc_auc_diff": roc_auc_score(self.y_train, y_pred_proba_train) - roc_auc_score(self.y_test, y_pred_proba_test),
                "f1_diff": f1_score(self.y_train, y_pred_train) - f1_score(self.y_test, y_pred_test),
                
                "classification_report": classification_report(self.y_test, y_pred_test)
            }

        return self.predictions
    
    def compare_models_performance(self):
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        comparison_data = []
        
        for model_name, metrics in self.predictions.items():
            comparison_data.append({
                'Model': model_name,
                'Test ROC-AUC': metrics['test_roc_auc'],
                'Test F1': metrics['test_f1_score'],
                'Train ROC-AUC': metrics['train_roc_auc'],
                'Train F1': metrics['train_f1_score'],
                'ROC-AUC Diff': metrics['roc_auc_diff'],
                'F1 Diff': metrics['f1_diff']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df = comparison_df.sort_values('Test ROC-AUC', ascending=False)
        
        return comparison_df
    
    def get_best_model(self, metric='test_roc_auc'):
        best_model_name = None
        best_score = -1
        
        for model_name, metrics in self.predictions.items():
            if metrics[metric] > best_score:
                best_score = metrics[metric]
                best_model_name = model_name
        
        return best_model_name, best_score
    
    def plot_roc_curve(self):
        """ –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ roc-auc –∫—Ä–∏–≤—ã—Ö """
        for name_model, model in self.models.items():
            y_pred_proba = model.predict_proba(self.X_test)[:,1]
        
            fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)
            roc_auc = auc(fpr, tpr)

            plt.plot(fpr, tpr, lw=2,
            label=f'{name_model} (AUC = {roc_auc:.3f})')

            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curves for {name_model}')
            plt.legend()
            plt.grid(True)
            plt.show()

    def optimize_classification_threshold(self, model_name, metric='f1'):
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏"""
        
        model = self.models[model_name]
        y_pred_proba = model.predict_proba(self.X_test)[:, 1]
        
        thresholds = np.arange(0.1, 0.9, 0.01)
        best_threshold = 0.5
        best_score = 0
        
        for threshold in thresholds:
            y_pred = (y_pred_proba >= threshold).astype(int)
            
            if metric == 'f1':
                score = f1_score(self.y_test, y_pred)
            elif metric == 'precision':
                score = precision_score(self.y_test, y_pred)
            elif metric == 'recall':
                score = recall_score(self.y_test, y_pred)
            
            if score > best_score:
                best_score = score
                best_threshold = threshold
        
        print(f"üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è {model_name}: {best_threshold:.3f}")
        print(f"   {metric.capitalize()} —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º: {best_score:.4f}")
        
        return best_threshold, best_score
    
    def evaluate_with_optimal_threshold(self, model_name, threshold):
        """–ü–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º"""
        model = self.models[model_name]
        y_pred_proba = model.predict_proba(self.X_test)[:, 1]
        y_pred_optimal = (y_pred_proba >= threshold).astype(int)
        
        optimal_metrics = {
            'f1_score': f1_score(self.y_test, y_pred_optimal),
            'precision': precision_score(self.y_test, y_pred_optimal),
            'recall': recall_score(self.y_test, y_pred_optimal),
            'roc_auc': roc_auc_score(self.y_test, y_pred_proba)
        }
        
        print(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ —Å –ø–æ—Ä–æ–≥–æ–º {threshold:.3f}:")
        for metric, value in optimal_metrics.items():
            print(f"   {metric}: {value:.4f}")
        
        return optimal_metrics
    
    def save_model(self, model_name, metrics=None):
        model = self.models[model_name]
        metadata = {
            'model_name': model,
            'training_date': datetime.now().strftime("%Y%m%d_%H%M%S"),
            'performance_metrics': self.predictions.get(model_name, {}),
        }

        if metrics:
            metadata.update(metrics)

        return self.model_manager.save_model(model, model_name, metadata)
    
    def load_model_in_trainer(self, model_name_or_path, new_name=None):
        model = self.model_manager.load_model(model_name_or_path)

        if new_name:
            model_name = new_name
        else:
            model_name = os.path.basename(model_name_or_path).split('_')[0]

        self.models[model_name] = model
        print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ trainer")
        return model

    def create_final_report(self, model_name, threshold=0.5):
        """–°–æ–∑–¥–∞—ë—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ –º–æ–¥–µ–ª–∏"""
        model = self.models[model_name]
        
        y_pred_proba = model.predict_proba(self.X_test)[:, 1]
        y_pred = (y_pred_proba >= threshold).astype(int)

        report = {
            'roc_auc': roc_auc_score(self.y_test, y_pred_proba),
            'f1': f1_score(self.y_test, y_pred),
            'precision': precision_score(self.y_test, y_pred),
            'recall': recall_score(self.y_test, y_pred),
            'classification_report': classification_report(self.y_test, y_pred),
            'confusion_matrix': confusion_matrix(self.y_test, y_pred),
            'threshold': threshold
        }
        
        return report

    def plot_confusion_matrix_final(self, model_name, threshold=0.5):        
        model = self.models[model_name]
        y_pred_proba = model.predict_proba(self.X_test)[:, 1]
        y_pred = (y_pred_proba >= threshold).astype(int)
        
        fig, ax = plt.subplots(figsize=(8, 6))
        ConfusionMatrixDisplay.from_predictions(self.y_test, y_pred, ax=ax, cmap='Blues')
        plt.title(f'Confusion Matrix ({model_name})\nThreshold = {threshold}')
        plt.show()
        
        tn, fp, fn, tp = confusion_matrix(self.y_test, y_pred).ravel()
        print("üìä –ë–∏–∑–Ω–µ—Å-–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:")
        print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∏ –ª–æ—è–ª—å–Ω—ã—Ö: {tn} –∫–ª–∏–µ–Ω—Ç–æ–≤")
        print(f"   –õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è (–Ω–∞–ø—Ä–∞—Å–Ω–æ –±–µ—Å–ø–æ–∫–æ–∏–ª–∏): {fp} –∫–ª–∏–µ–Ω—Ç–æ–≤") 
        print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —É—Ö–æ–¥—è—â–∏–µ: {fn} –∫–ª–∏–µ–Ω—Ç–æ–≤")
        print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —É—Ö–æ–¥—è—â–∏–µ: {tp} –∫–ª–∏–µ–Ω—Ç–æ–≤")

    def plot_feature_importance(self, model_name, top_n=15):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        model = self.models[model_name]
        
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            feature_names = self.X_train.columns if hasattr(self.X_train, 'columns') else [f'Feature_{i}' for i in range(len(importances))]
            
            indices = np.argsort(importances)[::-1]
            
            plt.figure(figsize=(10, 8))
            plt.title(f'Feature Importance - {model_name}')
            plt.barh(range(min(top_n, len(indices))), 
                    importances[indices][:top_n][::-1])
            plt.yticks(range(min(top_n, len(indices))), 
                    [feature_names[i] for i in indices[:top_n]][::-1])
            plt.xlabel('Importance')
            plt.tight_layout()
            plt.show()
            
            print("üéØ –¢–æ–ø-10 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            for i in range(min(10, len(indices))):
                print(f"   {i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}")
        else:
            print(f"‚ùå –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç feature importance")
